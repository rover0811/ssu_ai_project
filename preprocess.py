import json
import os
import whisper
import torch
import numpy as np
from pathlib import Path
from collections import Counter
import pickle
from multiprocessing import Pool, cpu_count
from functools import partial
import time


def pad_or_trim_audio(audio, target_length=30.0):
    """
    ì˜¤ë””ì˜¤ë¥¼ target_length ì´ˆë¡œ íŒ¨ë”©í•˜ê±°ë‚˜ íŠ¸ë¦¬ë°
    """
    target_samples = int(target_length * whisper.audio.SAMPLE_RATE)  # 16000 * 30 = 480000

    if len(audio) > target_samples:
        # íŠ¸ë¦¬ë°: ì²˜ìŒ 30ì´ˆë§Œ ì‚¬ìš©
        audio = audio[:target_samples]
    elif len(audio) < target_samples:
        # íŒ¨ë”©: 0ìœ¼ë¡œ ì±„ì›€
        padding_needed = target_samples - len(audio)
        audio = np.pad(audio, (0, padding_needed), mode='constant', constant_values=0)

    return audio


def pad_or_trim_mel(mel, target_frames=3000):
    """
    mel spectrogramì„ target_framesë¡œ íŒ¨ë”©í•˜ê±°ë‚˜ íŠ¸ë¦¬ë°
    mel shape: (80, time_frames)
    """
    current_frames = mel.shape[1]

    if current_frames > target_frames:
        # íŠ¸ë¦¬ë°
        mel = mel[:, :target_frames]
    elif current_frames < target_frames:
        # íŒ¨ë”©
        padding_needed = target_frames - current_frames
        mel = np.pad(mel, ((0, 0), (0, padding_needed)), mode='constant', constant_values=mel.min())

    return mel


def process_batch_whisper_features(batch_args):
    """
    ë°°ì¹˜ ë‹¨ìœ„ë¡œ Whisper ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ )
    í•œ ì›Œì»¤ê°€ ì—¬ëŸ¬ íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ëª¨ë¸ ë¡œë”© ì˜¤ë²„í—¤ë“œ ê°ì†Œ
    """
    batch_file_infos, model_name, target_length, all_rare_words = batch_args

    # ë°°ì¹˜ ì²˜ë¦¬ìš© ëª¨ë¸ ë¡œë“œ (í•œ ë²ˆë§Œ)
    model = whisper.load_model(model_name)
    target_frames = int(target_length * 100)  # 100 frames/sec

    results = []

    for i, file_info in enumerate(batch_file_infos):  # ğŸ‘ˆ i ì¶”ê°€
        try:
            if i % 10 == 0:
                print(f"    ë°°ì¹˜ ë‚´ ì§„í–‰: {i}/{len(batch_file_infos)} íŒŒì¼ ì²˜ë¦¬ ì¤‘...")

            # ì˜¤ë””ì˜¤ ì²˜ë¦¬
            audio = whisper.load_audio(file_info['voice_path'])
            audio = pad_or_trim_audio(audio, target_length=target_length)

            # mel spectrogram ìƒì„±
            mel = whisper.log_mel_spectrogram(audio)
            mel = pad_or_trim_mel(mel, target_frames=target_frames)

            # ë°”ì´ì–´ì‹± ë‹¨ì–´
            utterance_words = set(file_info['text'].split())
            bias_words = list(utterance_words.intersection(all_rare_words))

            mel_numpy = mel.numpy() if isinstance(mel, torch.Tensor) else mel

            results.append({
                'status': 'success',
                'fbank': mel_numpy,
                'words': file_info['text'],
                'blist': bias_words,
                'voice_path': file_info['voice_path']
            })

        except Exception as e:
            results.append({
                'status': 'error',
                'error': str(e),
                'voice_path': file_info['voice_path']
            })

    return results


def process_single_whisper_feature(args):
    """
    ë‹¨ì¼ íŒŒì¼ì˜ Whisper íŠ¹ì„± ì¶”ì¶œ (ë©€í‹°í”„ë¡œì„¸ì‹±ìš©)
    ê° ì›Œì»¤ì—ì„œ Whisper ëª¨ë¸ì„ ë³„ë„ë¡œ ë¡œë“œ
    """
    file_info, model_name, target_length, all_rare_words = args

    try:
        # ê° ì›Œì»¤ì—ì„œ ëª¨ë¸ì„ ë³„ë„ë¡œ ë¡œë“œ
        model = whisper.load_model(model_name)

        # 1. ì˜¤ë””ì˜¤ ë¡œë“œ
        audio = whisper.load_audio(file_info['voice_path'])

        # 2. ê¸¸ì´ í†µì¼ (íŒ¨ë”©/íŠ¸ë¦¬ë°)
        audio = pad_or_trim_audio(audio, target_length=target_length)

        # 3. mel spectrogram ìƒì„±
        mel = whisper.log_mel_spectrogram(audio)

        # 4. mel spectrogram ê¸¸ì´ í†µì¼
        target_frames = int(target_length * 100)  # 100 frames/sec
        mel = pad_or_trim_mel(mel, target_frames=target_frames)

        # 5. ë°”ì´ì–´ì‹± ë‹¨ì–´ ì¶”ì¶œ
        utterance_words = set(file_info['text'].split())
        bias_words = list(utterance_words.intersection(all_rare_words))

        # numpyë¡œ ë³€í™˜ (pickle í˜¸í™˜ì„±)
        mel_numpy = mel.numpy() if isinstance(mel, torch.Tensor) else mel

        return {
            'status': 'success',
            'fbank': mel_numpy,
            'words': file_info['text'],
            'blist': bias_words,
            'voice_path': file_info['voice_path']
        }

    except Exception as e:
        return {
            'status': 'error',
            'error': str(e),
            'voice_path': file_info['voice_path']
        }


def dump_feature_korean_tcpgen_format(training_dir, output_dir, num_workers=None, batch_size=50,
                                      use_batch_processing=True):
    """
    ì™„ì „ ë©€í‹°í”„ë¡œì„¸ì‹± ì ìš© TCPGEN WhisperBiasing í˜•ì‹ìœ¼ë¡œ mel spectrogram ìƒì„±
    ì¶œë ¥: fbank.pt íŒŒì¼ (í‚¤: fbank, words, blist)

    Args:
        training_dir: í›ˆë ¨ ë°ì´í„° ë””ë ‰í† ë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        num_workers: ì›Œì»¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜ì˜ 80%)
        batch_size: ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ë°°ì¹˜ í¬ê¸°
        use_batch_processing: Trueë©´ ë°°ì¹˜ ì²˜ë¦¬, Falseë©´ ê°œë³„ ì²˜ë¦¬
    """
    if num_workers is None:
        num_workers = max(1, int(cpu_count() * 0.8))  # CPU ì½”ì–´ì˜ 80% ì‚¬ìš©

    print(f"Using {num_workers} workers, batch_size={batch_size}, batch_processing={use_batch_processing}")

    # ê²½ë¡œ ì •ë³´ë§Œ ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ìˆ˜ì§‘
    label_dir = os.path.join(training_dir, "Label")
    json_files = list(Path(label_dir).glob("**/*.json"))

    args_list = [(json_file, training_dir) for json_file in json_files]

    # ê²½ë¡œ ì •ë³´ ìˆ˜ì§‘ (ë©€í‹°í”„ë¡œì„¸ì‹±)
    print("Collecting file information...")
    with Pool(num_workers) as pool:
        file_info_list = pool.map(process_single_file_light, args_list)

    valid_files = [info for info in file_info_list if info['status'] == 'success']
    print(f"Valid files: {len(valid_files)}")

    valid_files = valid_files[:len(valid_files) // 2]  # ì•ìª½ 50%ë§Œ ì‚¬ìš©

    # ë°”ì´ì–´ì‹± ë‹¨ì–´ ë¡œë“œ (ë¯¸ë¦¬ ìƒì„±ëœ í¬ê·€ ë‹¨ì–´ íŒŒì¼)
    rare_words_file = os.path.join(output_dir, 'korean_rareword_error.txt')
    if not os.path.exists(rare_words_file):
        print("Warning: í¬ê·€ ë‹¨ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € get_rarewords_korean_mp() ì‹¤í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return 0

    with open(rare_words_file, 'r', encoding='utf-8') as f:
        all_rare_words = set(word.strip() for word in f.readlines())

    # Whisper íŠ¹ì„± ì¶”ì¶œ (ì™„ì „ ë©€í‹°í”„ë¡œì„¸ì‹±)
    model_name = "small"
    target_length = 30.0

    print(f"Starting Whisper feature extraction with {num_workers} workers...")
    start_time = time.time()

    if use_batch_processing:
        # ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        batches = [valid_files[i:i + batch_size] for i in range(0, len(valid_files), batch_size)]
        batch_args = [(batch, model_name, target_length, all_rare_words) for batch in batches]

        print(f"Processing {len(batches)} batches...")

        with Pool(num_workers) as pool:
            batch_results = pool.map(process_batch_whisper_features, batch_args)

        # ë°°ì¹˜ ê²°ê³¼ í‰ë©´í™”
        all_results = []
        for batch_result in batch_results:
            all_results.extend(batch_result)

    else:
        # ê°œë³„ ì²˜ë¦¬ ë°©ì‹ (ë” ë§ì€ ë³‘ë ¬ì„±)
        feature_args = [(info, model_name, target_length, all_rare_words) for info in valid_files]

        print(f"Processing {len(feature_args)} files individually...")

        with Pool(num_workers) as pool:
            all_results = pool.map(process_single_whisper_feature, feature_args)

    processing_time = time.time() - start_time
    print(f"Whisper processing completed in {processing_time:.1f}s")

    # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§
    successful_results = [r for r in all_results if r['status'] == 'success']
    failed_results = [r for r in all_results if r['status'] == 'error']

    print(f"Successful: {len(successful_results)}, Failed: {len(failed_results)}")

    if failed_results:
        print("Failed files (first 5):")
        for fail in failed_results[:5]:
            print(f"  {fail['voice_path']}: {fail['error']}")

    if not successful_results:
        print("No successful results to save!")
        return 0

    # TCPGEN í˜•ì‹ìœ¼ë¡œ ì €ì¥
    fbank_features = [r['fbank'] for r in successful_results]
    words_list = [r['words'] for r in successful_results]
    blist_per_utterance = [r['blist'] for r in successful_results]

    # ì €ì¥ ì „ shape í™•ì¸
    if fbank_features:
        print(f"Final mel shapes: {[f.shape for f in fbank_features[:5]]}...")  # ì²˜ìŒ 5ê°œë§Œ í™•ì¸

    # TCPGEN í˜•ì‹ìœ¼ë¡œ ì €ì¥
    tcpgen_data = {
        'fbank': fbank_features,  # mel spectrogram ë¦¬ìŠ¤íŠ¸
        'words': words_list,  # í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        'blist': blist_per_utterance  # ê° ë°œí™”ë³„ ë°”ì´ì–´ì‹± ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
    }

    # .pt íŒŒì¼ë¡œ ì €ì¥ (TCPGENì´ torch.load()ë¡œ ì½ìŒ)
    output_file = os.path.join(output_dir, 'fbank.pt')
    torch.save(tcpgen_data, output_file)
    print(f"TCPGEN format data saved to: {output_file}")

    # ì¶”ê°€ë¡œ .pkl í˜•ì‹ë„ ì €ì¥ (í˜¸í™˜ì„±)
    pickle_file = os.path.join(output_dir, 'korean_lecture_features_tcpgen.pkl')
    with open(pickle_file, 'wb') as f:
        pickle.dump(tcpgen_data, f)
    print(f"Pickle format data saved to: {pickle_file}")

    # ì²« ë²ˆì§¸ ìƒ˜í”Œ ì •ë³´ ì¶œë ¥
    if fbank_features:
        print(f"Sample mel shape: {fbank_features[0].shape}")
        print(f"Sample text: {words_list[0][:100]}...")
        print(f"Sample bias words: {blist_per_utterance[0][:5]}")

    return len(fbank_features)


def create_tcpgen_json_files(training_dir, output_dir, rare_words_file):
    """
    TCPGENìš© JSON íŒŒì¼ ìƒì„±
    - train_clean_100_error.json í˜•íƒœ
    - ê° ë°œí™”ë³„ë¡œ ë°”ì´ì–´ì‹± ë‹¨ì–´ í¬í•¨
    """
    # í¬ê·€ ë‹¨ì–´ ë¡œë“œ
    with open(rare_words_file, 'r', encoding='utf-8') as f:
        rare_words = set(word.strip() for word in f.readlines())

    # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
    label_dir = os.path.join(training_dir, "Label")
    json_files = list(Path(label_dir).glob("**/*.json"))

    utterance_data = {}

    for i, json_file in enumerate(json_files):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            text = data["06_transcription"]["1_text"]
            file_id = data["01_dataset"]["1_identifier"]

            # ì´ ë°œí™”ì˜ ë°”ì´ì–´ì‹± ë‹¨ì–´
            utterance_words = set(text.split())
            bias_words = list(utterance_words.intersection(rare_words))

            if bias_words:  # ë°”ì´ì–´ì‹± ë‹¨ì–´ê°€ ìˆëŠ” ê²½ìš°ë§Œ í¬í•¨
                utterance_data[f"utterance_{i}"] = {
                    "text": text,
                    "bias_words": bias_words,
                    "file_id": file_id,
                    "json_path": str(json_file)
                }

        except Exception as e:
            continue

    # JSON íŒŒì¼ë¡œ ì €ì¥
    json_output_file = os.path.join(output_dir, 'korean_train_error.json')
    with open(json_output_file, 'w', encoding='utf-8') as f:
        json.dump(utterance_data, f, ensure_ascii=False, indent=2)

    print(f"TCPGEN JSON file created: {json_output_file}")
    print(f"Utterances with bias words: {len(utterance_data)}")

    return utterance_data


def validate_tcpgen_format(data_file):
    """
    TCPGEN í˜•ì‹ ë°ì´í„° ê²€ì¦
    """
    try:
        if data_file.endswith('.pt'):
            data = torch.load(data_file)
        else:
            with open(data_file, 'rb') as f:
                data = pickle.load(f)

        required_keys = ['fbank', 'words', 'blist']
        for key in required_keys:
            if key not in data:
                return False, f"Missing key: {key}"

        fbank = data['fbank']
        words = data['words']
        blist = data['blist']

        if len(fbank) != len(words) or len(words) != len(blist):
            return False, f"Length mismatch: fbank={len(fbank)}, words={len(words)}, blist={len(blist)}"

        # shape í™•ì¸
        if fbank:
            sample_shape = fbank[0].shape
            for i, feat in enumerate(fbank[:10]):  # ì²˜ìŒ 10ê°œë§Œ í™•ì¸
                if feat.shape != sample_shape:
                    return False, f"Shape mismatch at index {i}: {feat.shape} vs {sample_shape}"

        return True, f"Valid TCPGEN format: {len(fbank)} utterances, feature shape: {fbank[0].shape if fbank else 'N/A'}"

    except Exception as e:
        return False, f"Error loading file: {e}"


def get_rarewords_korean_mp(training_dir, output_dir, num_workers=None):
    """
    ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ì‚¬ìš©í•œ í¬ê·€ ë‹¨ì–´ ì¶”ì¶œ (TCPGENìš©)
    """
    if num_workers is None:
        num_workers = cpu_count()

    print(f"Using {num_workers} workers for text processing")

    os.makedirs(output_dir, exist_ok=True)

    label_dir = os.path.join(training_dir, "Label")
    json_files = list(Path(label_dir).glob("**/*.json"))
    print(f"Processing {len(json_files)} files for rare words")

    args_list = [(json_file, training_dir) for json_file in json_files]

    all_words = []
    major_words = {}
    utterance_data = []

    start_time = time.time()
    chunk_size = 500

    for i in range(0, len(args_list), chunk_size):
        chunk = args_list[i:i + chunk_size]

        with Pool(num_workers) as pool:
            results = pool.map(process_text_file, chunk)

        for result in results:
            if result['status'] == 'success':
                words = result['words']
                major = result['major']

                all_words.extend(words)

                if major not in major_words:
                    major_words[major] = []
                major_words[major].extend(words)

                utterance_data.append({
                    'text': result['text'],
                    'words': words,
                    'major': major,
                    'file_id': result['file_id']
                })

        elapsed = time.time() - start_time
        print(f"Text processed: {i + len(chunk)}/{len(json_files)} files ({elapsed:.1f}s)")

    word_freq = Counter(all_words)
    rare_words = [word for word, freq in word_freq.items() if freq <= 3 and len(word) > 1]

    # í•œêµ­ì–´ ê¸°ìˆ  ìš©ì–´ ì¶”ê°€
    tech_terms = {
        'comp': ['ì•Œê³ ë¦¬ì¦˜', 'ë°ì´í„°ë² ì´ìŠ¤', 'ê°ì²´ì§€í–¥', 'í”„ë¡œê·¸ë˜ë°', 'ì†Œí”„íŠ¸ì›¨ì–´', 'í•˜ë“œì›¨ì–´',
                 'ë„¤íŠ¸ì›Œí¬', 'ë³´ì•ˆ', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ë¹…ë°ì´í„°'],
        'eng': ['ë¯¸ì ë¶„í•™', 'ì„ í˜•ëŒ€ìˆ˜', 'ì—´ì—­í•™', 'ìœ ì²´ì—­í•™', 'íšŒë¡œì´ë¡ ', 'ì‹ í˜¸ì²˜ë¦¬',
                'ì œì–´ì‹œìŠ¤í…œ', 'êµ¬ì¡°ì—­í•™', 'ì¬ë£Œê³µí•™', 'ê¸°ê³„ì„¤ê³„'],
        'math': ['ë¯¸ë¶„ë°©ì •ì‹', 'í™•ë¥ ë¡ ', 'í†µê³„í•™', 'ì§‘í•©ë¡ ', 'ìœ„ìƒìˆ˜í•™', 'í•´ì„í•™',
                 'ëŒ€ìˆ˜í•™', 'ê¸°í•˜í•™', 'ìˆ˜ì¹˜í•´ì„'],
        'phy': ['ì–‘ìì—­í•™', 'ìƒëŒ€ì„±ì´ë¡ ', 'ì „ìê¸°í•™', 'ê³ ì²´ë¬¼ë¦¬', 'ì›ìë¬¼ë¦¬', 'í•µë¬¼ë¦¬']
    }

    major_rare_words = {}
    for major, words in major_words.items():
        major_freq = Counter(words)
        major_rare = [word for word, freq in major_freq.items() if freq <= 2 and len(word) > 1]
        major_rare_words[major] = major_rare

    final_rare_words = set(rare_words)

    for major, terms in tech_terms.items():
        final_rare_words.update(terms)
        if major in major_rare_words:
            final_rare_words.update(major_rare_words[major][:50])

    # TCPGEN í˜•ì‹ íŒŒì¼ ì €ì¥
    with open(os.path.join(output_dir, 'korean_rareword_error.txt'), 'w', encoding='utf-8') as f:
        for word in sorted(final_rare_words):
            f.write(f"{word}\n")

    with open(os.path.join(output_dir, 'korean_all_rare_words.txt'), 'w', encoding='utf-8') as f:
        for word in sorted(final_rare_words):
            f.write(f"{word}\n")

    # ë°œí™”ë³„ í¸í–¥ ë‹¨ì–´ ìƒì„± (TCPGEN JSON í˜•ì‹)
    utterance_bias = {}
    for i, utt_data in enumerate(utterance_data):
        bias_words = [word for word in utt_data['words'] if word in final_rare_words]

        if bias_words:
            utterance_bias[f"utterance_{i}"] = {
                "text": utt_data['text'],
                "bias_words": bias_words,
                "major": utt_data['major'],
                "file_id": utt_data['file_id']
            }

    with open(os.path.join(output_dir, 'korean_train_error.json'), 'w', encoding='utf-8') as f:
        json.dump(utterance_bias, f, ensure_ascii=False, indent=2)

    print(f"Total rare words: {len(final_rare_words)}")
    print(f"Utterances with bias words: {len(utterance_bias)}")
    print(f"Text processing completed in {time.time() - start_time:.1f}s")

    return final_rare_words, utterance_bias


def process_text_file(args):
    """í…ìŠ¤íŠ¸ ì²˜ë¦¬ìš© ë‹¨ì¼ íŒŒì¼ í•¨ìˆ˜ (ë©€í‹°í”„ë¡œì„¸ì‹±ìš©)"""
    json_file, training_dir = args

    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        text = data["06_transcription"]["1_text"]
        major = data["03_lectureinfo"]["3_major_category"]
        file_id = data["01_dataset"]["1_identifier"]

        words = text.split()

        return {
            'words': words,
            'major': major,
            'text': text,
            'file_id': file_id,
            'status': 'success'
        }

    except Exception as e:
        return {'status': 'error', 'file': str(json_file), 'error': str(e)}


def process_single_file_light(args):
    """ë©”ëª¨ë¦¬ ì ˆì•½ ë²„ì „ - Whisper ëª¨ë¸ ë¡œë“œ ì—†ì´ ê²½ë¡œë§Œ í™•ì¸"""
    json_file, training_dir = args

    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        audio_relative_path = data["01_dataset"]["3_src_path"]
        text = data["06_transcription"]["1_text"]
        major = data["03_lectureinfo"]["3_major_category"]
        file_id = data["01_dataset"]["1_identifier"]

        audio_file_path = '/'.join(audio_relative_path.split('/')[-2:])
        voice_path = os.path.join(training_dir, "Voice", audio_file_path)

        if os.path.exists(voice_path):
            return {
                'voice_path': voice_path,
                'text': text,
                'major': major,
                'file_id': file_id,
                'status': 'success'
            }
        else:
            return {'status': 'audio_not_found', 'file': str(json_file)}

    except Exception as e:
        return {'status': 'error', 'file': str(json_file), 'error': str(e)}


# TCPGENìš© ì „ì²´ íŒŒì´í”„ë¼ì¸
def run_tcpgen_preprocessing_pipeline(training_dir, output_dir, num_workers=None, batch_size=50,
                                      use_batch_processing=True):
    """
    ì™„ì „ ë©€í‹°í”„ë¡œì„¸ì‹± ì ìš© TCPGEN WhisperBiasingìš© ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    1. í¬ê·€ ë‹¨ì–´ ì¶”ì¶œ
    2. TCPGEN í˜•ì‹ íŠ¹ì§• ì¶”ì¶œ (ë©€í‹°í”„ë¡œì„¸ì‹±)
    3. JSON íŒŒì¼ ìƒì„±
    4. ê²€ì¦

    Args:
        training_dir: í›ˆë ¨ ë°ì´í„° ë””ë ‰í† ë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        num_workers: ì›Œì»¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜ì˜ 80%)
        batch_size: ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ë°°ì¹˜ í¬ê¸°
        use_batch_processing: Trueë©´ ë°°ì¹˜ ì²˜ë¦¬, Falseë©´ ê°œë³„ ì²˜ë¦¬
    """

    print("=" * 60)
    print("TCPGEN WHISPERBIASING ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ì™„ì „ ë©€í‹°í”„ë¡œì„¸ì‹±)")
    print("=" * 60)

    if num_workers is None:
        num_workers = max(1, int(cpu_count() * 0.8))

    print(f"Using {num_workers} workers (CPU cores: {cpu_count()})")
    print(f"Batch processing: {use_batch_processing}, Batch size: {batch_size}")

    os.makedirs(output_dir, exist_ok=True)

    # 1. í¬ê·€ ë‹¨ì–´ ì¶”ì¶œ
    print("\n1. í¬ê·€ ë‹¨ì–´ ì¶”ì¶œ ì¤‘...")
    start_time = time.time()
    rare_words, bias_data = get_rarewords_korean_mp(training_dir, output_dir, num_workers)
    text_time = time.time() - start_time
    print(f"í¬ê·€ ë‹¨ì–´ ì¶”ì¶œ ì™„ë£Œ: {len(rare_words)}ê°œ ë‹¨ì–´, {text_time:.1f}ì´ˆ")

    # 2. TCPGEN í˜•ì‹ íŠ¹ì§• ì¶”ì¶œ (ì™„ì „ ë©€í‹°í”„ë¡œì„¸ì‹±)
    print("\n2. TCPGEN í˜•ì‹ íŠ¹ì§• ì¶”ì¶œ ì¤‘ (ë©€í‹°í”„ë¡œì„¸ì‹±)...")
    start_time = time.time()
    total_processed = dump_feature_korean_tcpgen_format(
        training_dir, output_dir, num_workers, batch_size, use_batch_processing
    )
    feature_time = time.time() - start_time
    print(f"íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {total_processed}ê°œ íŒŒì¼, {feature_time:.1f}ì´ˆ")

    if total_processed == 0:
        print("íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨! íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return 0

    # 3. ê²€ì¦
    print("\n3. TCPGEN í˜•ì‹ ê²€ì¦ ì¤‘...")
    fbank_file = os.path.join(output_dir, 'fbank.pt')
    is_valid, msg = validate_tcpgen_format(fbank_file)
    print(f"ê²€ì¦ ê²°ê³¼: {'âœ… ì„±ê³µ' if is_valid else 'âŒ ì‹¤íŒ¨'}")
    print(f"ì„¸ë¶€ì‚¬í•­: {msg}")

    # 4. ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 60)
    print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {feature_time + text_time:.1f}ì´ˆ")
    print(f"Whisper ì²˜ë¦¬ ì‹œê°„: {feature_time:.1f}ì´ˆ (ë©€í‹°í”„ë¡œì„¸ì‹± ì ìš©)")
    print(f"í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹œê°„: {text_time:.1f}ì´ˆ")
    print(f"ì†ë„ í–¥ìƒ: Whisper ì²˜ë¦¬ê°€ {num_workers}ê°œ ì›Œì»¤ë¡œ ë³‘ë ¬í™”ë¨")
    print(f"ìƒì„±ëœ íŒŒì¼ë“¤:")
    print(f"  - fbank.pt (TCPGEN ë©”ì¸ ë°ì´í„°)")
    print(f"  - korean_rareword_error.txt (ë°”ì´ì–´ì‹± ë‹¨ì–´ ëª©ë¡)")
    print(f"  - korean_train_error.json (ë°œí™”ë³„ ë°”ì´ì–´ì‹± ë‹¨ì–´)")
    print(f"  - korean_lecture_features_tcpgen.pkl (í˜¸í™˜ì„±ìš©)")

    return total_processed


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    training_dir = "./IT_Lecture/Training"
    output_dir = "./korean_processed_tcpgen"

    num_workers = max(1, int(cpu_count() * 0.8))  # CPU ì½”ì–´ì˜ 80% ì‚¬ìš©
    print(f"Available CPU cores: {cpu_count()}, Using {num_workers} workers")

    label_dir = os.path.join(training_dir, "Label")
    voice_dir = os.path.join(training_dir, "Voice")

    if not os.path.exists(label_dir):
        print(f"Error: Label directory not found at {label_dir}")
        exit(1)
    if not os.path.exists(voice_dir):
        print(f"Error: Voice directory not found at {voice_dir}")
        exit(1)

    print(f"Label directory: {label_dir}")
    print(f"Voice directory: {voice_dir}")

    # TCPGEN ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì™„ì „ ë©€í‹°í”„ë¡œì„¸ì‹±)
    total_processed = run_tcpgen_preprocessing_pipeline(
        training_dir,
        output_dir,
        num_workers=num_workers,
        batch_size=30,  # ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ
        use_batch_processing=True  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
    )

    print(f"\nğŸ‰ TCPGEN ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ì´ì œ WhisperBiasing ë ˆí¬ì˜ train.pyë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ê°œë³„ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ì‹¶ë‹¤ë©´:
    # total_processed = run_tcpgen_preprocessing_pipeline(
    #     training_dir,
    #     output_dir,
    #     num_workers=4,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìœ¼ë¯€ë¡œ ì›Œì»¤ ìˆ˜ ì¤„ì„
    #     use_batch_processing=False
    # )