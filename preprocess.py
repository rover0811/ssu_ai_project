import json
import os
import whisper
import torch
import numpy as np
from pathlib import Path
from collections import Counter
import pickle
from multiprocessing import Pool, cpu_count
from functools import partial
import time


def dump_feature_korean_tcpgen_format(training_dir, output_dir, num_workers=None):
    """
    TCPGEN WhisperBiasing í˜•ì‹ìœ¼ë¡œ mel spectrogram ìƒì„±
    ì¶œë ¥: fbank.pt íŒŒì¼ (í‚¤: fbank, words, blist)
    """
    if num_workers is None:
        num_workers = 6

    # Whisper ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œ
    model = whisper.load_model("medium")

    # ê¸¸ì´ ì„¤ì • (Whisper ê¸°ë³¸ê°’)
    TARGET_LENGTH = 3000  # 30ì´ˆ * 100 frames/sec = 3000 frames

    # ê²½ë¡œ ì •ë³´ë§Œ ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ìˆ˜ì§‘
    label_dir = os.path.join(training_dir, "Label")
    json_files = list(Path(label_dir).glob("**/*.json"))

    args_list = [(json_file, training_dir) for json_file in json_files]

    # ê²½ë¡œ ì •ë³´ ìˆ˜ì§‘ (ë©€í‹°í”„ë¡œì„¸ì‹±)
    with Pool(num_workers) as pool:
        file_info_list = pool.map(process_single_file_light, args_list)

    valid_files = [info for info in file_info_list if info['status'] == 'success']

    # ë°”ì´ì–´ì‹± ë‹¨ì–´ ë¡œë“œ (ë¯¸ë¦¬ ìƒì„±ëœ í¬ê·€ ë‹¨ì–´ íŒŒì¼)
    rare_words_file = os.path.join(output_dir, 'korean_rareword_error.txt')
    if not os.path.exists(rare_words_file):
        print("Warning: í¬ê·€ ë‹¨ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € get_rarewords_korean_mp() ì‹¤í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return 0

    with open(rare_words_file, 'r', encoding='utf-8') as f:
        all_rare_words = set(word.strip() for word in f.readlines())

    # ìˆœì°¨ì ìœ¼ë¡œ Whisper ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
    fbank_features = []  # mel spectrogram
    words_list = []  # ì „ì²´ í…ìŠ¤íŠ¸
    blist_per_utterance = []  # ê° ë°œí™”ë³„ ë°”ì´ì–´ì‹± ë‹¨ì–´

    for i, info in enumerate(valid_files):
        try:
            # 1. ì˜¤ë””ì˜¤ ë¡œë“œ
            audio = whisper.load_audio(info['voice_path'])

            # 2. ê¸¸ì´ í†µì¼ (íŒ¨ë”©/íŠ¸ë¦¬ë°)
            audio = pad_or_trim_audio(audio, target_length=30.0)  # 30ì´ˆë¡œ í†µì¼

            # 3. mel spectrogram ìƒì„±
            mel = whisper.log_mel_spectrogram(audio)

            # 4. mel spectrogramë„ ê¸¸ì´ í™•ì¸/í†µì¼
            mel = pad_or_trim_mel(mel, target_frames=TARGET_LENGTH)

            # 5. ì´ ë°œí™”ì˜ ë°”ì´ì–´ì‹± ë‹¨ì–´ ì¶”ì¶œ
            utterance_words = set(info['text'].split())
            bias_words_for_this_utterance = list(utterance_words.intersection(all_rare_words))

            # 6. TCPGEN í˜•ì‹ìœ¼ë¡œ ì €ì¥
            fbank_features.append(mel.numpy() if isinstance(mel, torch.Tensor) else mel)
            words_list.append(info['text'])
            blist_per_utterance.append(bias_words_for_this_utterance)

            if i % 100 == 0:
                print(
                    f"Processed {i}/{len(valid_files)} files - mel shape: {mel.shape}, bias words: {len(bias_words_for_this_utterance)}")

        except Exception as e:
            print(f"Error processing {info['voice_path']}: {e}")
            continue

    # ì €ì¥ ì „ shape í™•ì¸
    if fbank_features:
        print(f"Final mel shapes: {[f.shape for f in fbank_features[:5]]}...")  # ì²˜ìŒ 5ê°œë§Œ í™•ì¸

    # TCPGEN í˜•ì‹ìœ¼ë¡œ ì €ì¥
    tcpgen_data = {
        'fbank': fbank_features,  # mel spectrogram ë¦¬ìŠ¤íŠ¸
        'words': words_list,  # í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        'blist': blist_per_utterance  # ê° ë°œí™”ë³„ ë°”ì´ì–´ì‹± ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
    }

    # .pt íŒŒì¼ë¡œ ì €ì¥ (TCPGENì´ torch.load()ë¡œ ì½ìŒ)
    output_file = os.path.join(output_dir, 'fbank.pt')
    torch.save(tcpgen_data, output_file)
    print(f"TCPGEN format data saved to: {output_file}")

    # ì¶”ê°€ë¡œ .pkl í˜•ì‹ë„ ì €ì¥ (í˜¸í™˜ì„±)
    pickle_file = os.path.join(output_dir, 'korean_lecture_features_tcpgen.pkl')
    with open(pickle_file, 'wb') as f:
        pickle.dump(tcpgen_data, f)
    print(f"Pickle format data saved to: {pickle_file}")

    return len(fbank_features)


def create_tcpgen_json_files(training_dir, output_dir, rare_words_file):
    """
    TCPGENìš© JSON íŒŒì¼ ìƒì„±
    - train_clean_100_error.json í˜•íƒœ
    - ê° ë°œí™”ë³„ë¡œ ë°”ì´ì–´ì‹± ë‹¨ì–´ í¬í•¨
    """
    # í¬ê·€ ë‹¨ì–´ ë¡œë“œ
    with open(rare_words_file, 'r', encoding='utf-8') as f:
        rare_words = set(word.strip() for word in f.readlines())

    # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
    label_dir = os.path.join(training_dir, "Label")
    json_files = list(Path(label_dir).glob("**/*.json"))

    utterance_data = {}

    for i, json_file in enumerate(json_files):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            text = data["06_transcription"]["1_text"]
            file_id = data["01_dataset"]["1_identifier"]

            # ì´ ë°œí™”ì˜ ë°”ì´ì–´ì‹± ë‹¨ì–´
            utterance_words = set(text.split())
            bias_words = list(utterance_words.intersection(rare_words))

            if bias_words:  # ë°”ì´ì–´ì‹± ë‹¨ì–´ê°€ ìˆëŠ” ê²½ìš°ë§Œ í¬í•¨
                utterance_data[f"utterance_{i}"] = {
                    "text": text,
                    "bias_words": bias_words,
                    "file_id": file_id,
                    "json_path": str(json_file)
                }

        except Exception as e:
            continue

    # JSON íŒŒì¼ë¡œ ì €ì¥
    json_output_file = os.path.join(output_dir, 'korean_train_error.json')
    with open(json_output_file, 'w', encoding='utf-8') as f:
        json.dump(utterance_data, f, ensure_ascii=False, indent=2)

    print(f"TCPGEN JSON file created: {json_output_file}")
    print(f"Utterances with bias words: {len(utterance_data)}")

    return utterance_data


def pad_or_trim_audio(audio, target_length=30.0):
    """
    ì˜¤ë””ì˜¤ë¥¼ target_length ì´ˆë¡œ íŒ¨ë”©í•˜ê±°ë‚˜ íŠ¸ë¦¬ë°
    """
    target_samples = int(target_length * whisper.audio.SAMPLE_RATE)  # 16000 * 30 = 480000

    if len(audio) > target_samples:
        # íŠ¸ë¦¬ë°: ì²˜ìŒ 30ì´ˆë§Œ ì‚¬ìš©
        audio = audio[:target_samples]
    elif len(audio) < target_samples:
        # íŒ¨ë”©: 0ìœ¼ë¡œ ì±„ì›€
        padding_needed = target_samples - len(audio)
        audio = np.pad(audio, (0, padding_needed), mode='constant', constant_values=0)

    return audio


def pad_or_trim_mel(mel, target_frames=3000):
    """
    mel spectrogramì„ target_framesë¡œ íŒ¨ë”©í•˜ê±°ë‚˜ íŠ¸ë¦¬ë°
    mel shape: (80, time_frames)
    """
    current_frames = mel.shape[1]

    if current_frames > target_frames:
        # íŠ¸ë¦¬ë°
        mel = mel[:, :target_frames]
    elif current_frames < target_frames:
        # íŒ¨ë”©
        padding_needed = target_frames - current_frames
        mel = np.pad(mel, ((0, 0), (0, padding_needed)), mode='constant', constant_values=mel.min())

    return mel


def validate_tcpgen_format(data_file):
    """
    TCPGEN í˜•ì‹ ë°ì´í„° ê²€ì¦
    """
    try:
        if data_file.endswith('.pt'):
            data = torch.load(data_file)
        else:
            with open(data_file, 'rb') as f:
                data = pickle.load(f)

        required_keys = ['fbank', 'words', 'blist']
        for key in required_keys:
            if key not in data:
                return False, f"Missing key: {key}"

        fbank = data['fbank']
        words = data['words']
        blist = data['blist']

        if len(fbank) != len(words) or len(words) != len(blist):
            return False, f"Length mismatch: fbank={len(fbank)}, words={len(words)}, blist={len(blist)}"

        # shape í™•ì¸
        if fbank:
            sample_shape = fbank[0].shape
            for i, feat in enumerate(fbank[:10]):  # ì²˜ìŒ 10ê°œë§Œ í™•ì¸
                if feat.shape != sample_shape:
                    return False, f"Shape mismatch at index {i}: {feat.shape} vs {sample_shape}"

        return True, f"Valid TCPGEN format: {len(fbank)} utterances, feature shape: {fbank[0].shape if fbank else 'N/A'}"

    except Exception as e:
        return False, f"Error loading file: {e}"


def get_rarewords_korean_mp(training_dir, output_dir, num_workers=None):
    """
    ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ì‚¬ìš©í•œ í¬ê·€ ë‹¨ì–´ ì¶”ì¶œ (TCPGENìš©)
    """
    if num_workers is None:
        num_workers = cpu_count()

    print(f"Using {num_workers} workers for text processing")

    os.makedirs(output_dir, exist_ok=True)

    label_dir = os.path.join(training_dir, "Label")
    json_files = list(Path(label_dir).glob("**/*.json"))
    print(f"Processing {len(json_files)} files for rare words")

    args_list = [(json_file, training_dir) for json_file in json_files]

    all_words = []
    major_words = {}
    utterance_data = []

    start_time = time.time()
    chunk_size = 500

    for i in range(0, len(args_list), chunk_size):
        chunk = args_list[i:i + chunk_size]

        with Pool(num_workers) as pool:
            results = pool.map(process_text_file, chunk)

        for result in results:
            if result['status'] == 'success':
                words = result['words']
                major = result['major']

                all_words.extend(words)

                if major not in major_words:
                    major_words[major] = []
                major_words[major].extend(words)

                utterance_data.append({
                    'text': result['text'],
                    'words': words,
                    'major': major,
                    'file_id': result['file_id']
                })

        elapsed = time.time() - start_time
        print(f"Text processed: {i + len(chunk)}/{len(json_files)} files ({elapsed:.1f}s)")

    word_freq = Counter(all_words)
    rare_words = [word for word, freq in word_freq.items() if freq <= 3 and len(word) > 1]

    # í•œêµ­ì–´ ê¸°ìˆ  ìš©ì–´ ì¶”ê°€
    tech_terms = {
        'comp': ['ì•Œê³ ë¦¬ì¦˜', 'ë°ì´í„°ë² ì´ìŠ¤', 'ê°ì²´ì§€í–¥', 'í”„ë¡œê·¸ë˜ë°', 'ì†Œí”„íŠ¸ì›¨ì–´', 'í•˜ë“œì›¨ì–´',
                 'ë„¤íŠ¸ì›Œí¬', 'ë³´ì•ˆ', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ë¹…ë°ì´í„°'],
        'eng': ['ë¯¸ì ë¶„í•™', 'ì„ í˜•ëŒ€ìˆ˜', 'ì—´ì—­í•™', 'ìœ ì²´ì—­í•™', 'íšŒë¡œì´ë¡ ', 'ì‹ í˜¸ì²˜ë¦¬',
                'ì œì–´ì‹œìŠ¤í…œ', 'êµ¬ì¡°ì—­í•™', 'ì¬ë£Œê³µí•™', 'ê¸°ê³„ì„¤ê³„'],
        'math': ['ë¯¸ë¶„ë°©ì •ì‹', 'í™•ë¥ ë¡ ', 'í†µê³„í•™', 'ì§‘í•©ë¡ ', 'ìœ„ìƒìˆ˜í•™', 'í•´ì„í•™',
                 'ëŒ€ìˆ˜í•™', 'ê¸°í•˜í•™', 'ìˆ˜ì¹˜í•´ì„'],
        'phy': ['ì–‘ìì—­í•™', 'ìƒëŒ€ì„±ì´ë¡ ', 'ì „ìê¸°í•™', 'ê³ ì²´ë¬¼ë¦¬', 'ì›ìë¬¼ë¦¬', 'í•µë¬¼ë¦¬']
    }

    major_rare_words = {}
    for major, words in major_words.items():
        major_freq = Counter(words)
        major_rare = [word for word, freq in major_freq.items() if freq <= 2 and len(word) > 1]
        major_rare_words[major] = major_rare

    final_rare_words = set(rare_words)

    for major, terms in tech_terms.items():
        final_rare_words.update(terms)
        if major in major_rare_words:
            final_rare_words.update(major_rare_words[major][:50])

    # TCPGEN í˜•ì‹ íŒŒì¼ ì €ì¥
    with open(os.path.join(output_dir, 'korean_rareword_error.txt'), 'w', encoding='utf-8') as f:
        for word in sorted(final_rare_words):
            f.write(f"{word}\n")

    with open(os.path.join(output_dir, 'korean_all_rare_words.txt'), 'w', encoding='utf-8') as f:
        for word in sorted(final_rare_words):
            f.write(f"{word}\n")

    # ë°œí™”ë³„ í¸í–¥ ë‹¨ì–´ ìƒì„± (TCPGEN JSON í˜•ì‹)
    utterance_bias = {}
    for i, utt_data in enumerate(utterance_data):
        bias_words = [word for word in utt_data['words'] if word in final_rare_words]

        if bias_words:
            utterance_bias[f"utterance_{i}"] = {
                "text": utt_data['text'],
                "bias_words": bias_words,
                "major": utt_data['major'],
                "file_id": utt_data['file_id']
            }

    with open(os.path.join(output_dir, 'korean_train_error.json'), 'w', encoding='utf-8') as f:
        json.dump(utterance_bias, f, ensure_ascii=False, indent=2)

    print(f"Total rare words: {len(final_rare_words)}")
    print(f"Utterances with bias words: {len(utterance_bias)}")
    print(f"Text processing completed in {time.time() - start_time:.1f}s")

    return final_rare_words, utterance_bias


def process_text_file(args):
    """í…ìŠ¤íŠ¸ ì²˜ë¦¬ìš© ë‹¨ì¼ íŒŒì¼ í•¨ìˆ˜ (ë©€í‹°í”„ë¡œì„¸ì‹±ìš©)"""
    json_file, training_dir = args

    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        text = data["06_transcription"]["1_text"]
        major = data["03_lectureinfo"]["3_major_category"]
        file_id = data["01_dataset"]["1_identifier"]

        words = text.split()

        return {
            'words': words,
            'major': major,
            'text': text,
            'file_id': file_id,
            'status': 'success'
        }

    except Exception as e:
        return {'status': 'error', 'file': str(json_file), 'error': str(e)}


def process_single_file_light(args):
    """ë©”ëª¨ë¦¬ ì ˆì•½ ë²„ì „ - Whisper ëª¨ë¸ ë¡œë“œ ì—†ì´ ê²½ë¡œë§Œ í™•ì¸"""
    json_file, training_dir = args

    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        audio_relative_path = data["01_dataset"]["3_src_path"]
        text = data["06_transcription"]["1_text"]
        major = data["03_lectureinfo"]["3_major_category"]
        file_id = data["01_dataset"]["1_identifier"]

        audio_file_path = '/'.join(audio_relative_path.split('/')[-2:])
        voice_path = os.path.join(training_dir, "Voice", audio_file_path)

        if os.path.exists(voice_path):
            return {
                'voice_path': voice_path,
                'text': text,
                'major': major,
                'file_id': file_id,
                'status': 'success'
            }
        else:
            return {'status': 'audio_not_found', 'file': str(json_file)}

    except Exception as e:
        return {'status': 'error', 'file': str(json_file), 'error': str(e)}


# TCPGENìš© ì „ì²´ íŒŒì´í”„ë¼ì¸
def run_tcpgen_preprocessing_pipeline(training_dir, output_dir, num_workers=None):
    """
    TCPGEN WhisperBiasingìš© ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    1. í¬ê·€ ë‹¨ì–´ ì¶”ì¶œ
    2. TCPGEN í˜•ì‹ íŠ¹ì§• ì¶”ì¶œ
    3. JSON íŒŒì¼ ìƒì„±
    4. ê²€ì¦
    """

    print("=" * 60)
    print("TCPGEN WHISPERBIASING ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸")
    print("=" * 60)

    os.makedirs(output_dir, exist_ok=True)

    # 1. í¬ê·€ ë‹¨ì–´ ì¶”ì¶œ
    print("\n1. í¬ê·€ ë‹¨ì–´ ì¶”ì¶œ ì¤‘...")
    start_time = time.time()
    rare_words, bias_data = get_rarewords_korean_mp(training_dir, output_dir, num_workers)
    text_time = time.time() - start_time
    print(f"í¬ê·€ ë‹¨ì–´ ì¶”ì¶œ ì™„ë£Œ: {len(rare_words)}ê°œ ë‹¨ì–´, {text_time:.1f}ì´ˆ")

    # 2. TCPGEN í˜•ì‹ íŠ¹ì§• ì¶”ì¶œ
    print("\n2. TCPGEN í˜•ì‹ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    start_time = time.time()
    total_processed = dump_feature_korean_tcpgen_format(training_dir, output_dir, num_workers)
    feature_time = time.time() - start_time
    print(f"íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {total_processed}ê°œ íŒŒì¼, {feature_time:.1f}ì´ˆ")

    # 3. ê²€ì¦
    print("\n3. TCPGEN í˜•ì‹ ê²€ì¦ ì¤‘...")
    fbank_file = os.path.join(output_dir, 'fbank.pt')
    is_valid, msg = validate_tcpgen_format(fbank_file)
    print(f"ê²€ì¦ ê²°ê³¼: {'âœ… ì„±ê³µ' if is_valid else 'âŒ ì‹¤íŒ¨'}")
    print(f"ì„¸ë¶€ì‚¬í•­: {msg}")

    # 4. ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 60)
    print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {feature_time + text_time:.1f}ì´ˆ")
    print(f"ìƒì„±ëœ íŒŒì¼ë“¤:")
    print(f"  - fbank.pt (TCPGEN ë©”ì¸ ë°ì´í„°)")
    print(f"  - korean_rareword_error.txt (ë°”ì´ì–´ì‹± ë‹¨ì–´ ëª©ë¡)")
    print(f"  - korean_train_error.json (ë°œí™”ë³„ ë°”ì´ì–´ì‹± ë‹¨ì–´)")
    print(f"  - korean_lecture_features_tcpgen.pkl (í˜¸í™˜ì„±ìš©)")

    return total_processed


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    training_dir = "./IT_Lecture/Training"
    output_dir = "./korean_processed_tcpgen"

    num_workers = cpu_count()
    print(f"Available CPU cores: {cpu_count()}, Using {num_workers} workers")

    label_dir = os.path.join(training_dir, "Label")
    voice_dir = os.path.join(training_dir, "Voice")

    if not os.path.exists(label_dir):
        print(f"Error: Label directory not found at {label_dir}")
        exit(1)
    if not os.path.exists(voice_dir):
        print(f"Error: Voice directory not found at {voice_dir}")
        exit(1)

    print(f"Label directory: {label_dir}")
    print(f"Voice directory: {voice_dir}")

    # TCPGEN ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    total_processed = run_tcpgen_preprocessing_pipeline(training_dir, output_dir, num_workers)

    print(f"\nğŸ‰ TCPGEN ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ì´ì œ WhisperBiasing ë ˆí¬ì˜ train.pyë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")